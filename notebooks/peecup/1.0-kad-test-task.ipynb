{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    \n",
    "__Регрессия__\n",
    "    \n",
    "В общем то как я и говорил делаешь то же самое что и для тестового задания: регрессию по концентрации для каждой из солей. Любым методом, какой больше нравится / лучше работает.\n",
    "Метрики те же : MAE, RMSE, $R^2$\n",
    "\n",
    "Единственное что для нормальной оценки, нужно делать проверку не на отложенной выборке с помощью `train_test_split`, а по кросс-валидации. \n",
    "    \n",
    "А именно, для поиска гиперпараметров есть функция `GridSearchCV`. Документация у `sklearn` довольно понятная, а потому искренне советую туда заглянуть.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    \n",
    "__Классификация__\n",
    "   \n",
    "Сначала нужно привести эту задачу к задаче классификации: ввести пороговое значение, все концентрации что ниже порога получают метку \"0\", все что выше - метку \"1\".\n",
    "    \n",
    "А дальше подбор параметров на кросс-валидации, со стратификацией по таргету, так же чтобы количество единичек и нулей на тесте и трейне было одинаковым\n",
    "    \n",
    "Метрики качества : precision, recall, f1-score, ROC-AUC (последнее использовать только если у тебя порог по концетрации не такой, что слишком мало или слишком много какого то из классов (из-за большого дисбаланса классов метрика ведет себя не очень адекватно)). Лучше всего первые три показать после подбора гиперпараметров с помощью `classification_report`:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта штука генерирует набор объектов и меток\n",
    "# Типа \"модельная\" классификация\n",
    "X, y = make_classification(n_samples=1000)\n",
    "Train_X, Test_X, Train_y, Test_y = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(Train_X, Train_y)\n",
    "\n",
    "# Обрати внимание что есть \"probs\" а есть \"preds\"\n",
    "# \"probs\" - это вероятности принадлежности классу\n",
    "# \"preds\" - непосредственно метки класса\n",
    "# У тебя \"probs\" - это значения функции сигмоиды от выхода PLS модели (для мочи)\n",
    "# а \"preds\" - это то что после отсечения по threshold\n",
    "\n",
    "probs = clf.predict_proba(Test_X)[:,1]\n",
    "preds = clf.predict(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       102\n",
      "           1       0.96      0.87      0.91        98\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.92      0.91      0.91       200\n",
      "weighted avg       0.92      0.92      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Также обрати внимание что в roc_auc_score передается именно вероятности probs\n",
    "print(f\"ROC-AUC: {roc_auc_score(Test_y, probs):.3f}\")\n",
    "print(classification_report(Test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    \n",
    "Делаешь то же самое только на кросс-валидации\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    \n",
    "Для адекватного использования этого вместе с `GridSearchCV` есть штука `pipeline`. Также рекомендую посмотреть примеры использования в документации\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
